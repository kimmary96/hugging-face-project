from unsloth import FastLanguageModel
import torch
import json

# ==========================================
# 1. ì„¤ì •
# ==========================================
# ë°©ê¸ˆ í•™ìŠµ ëë‚œ ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
ADAPTER_PATH = "outputs_checkpoint" 
MAX_SEQ_LENGTH = 2048

# ==========================================
# 2. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (Base + Adapter)
# ==========================================
print(f"ğŸ”„ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘... ({ADAPTER_PATH})")

# UnslothëŠ” ì €ì¥ëœ í´ë” ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ Base ëª¨ë¸ + LoRAë¥¼ í•©ì³ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = ADAPTER_PATH, 
    max_seq_length = MAX_SEQ_LENGTH,
    dtype = None,
    load_in_4bit = True,
)

# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜ (ì†ë„ 2ë°° í–¥ìƒ)
FastLanguageModel.for_inference(model)

# ==========================================
# 3. í…ŒìŠ¤íŠ¸í•  ê°€ìƒì˜ ìœ ì €ë“¤ (New Personas)
# ==========================================
test_users = [
    # Case 1: 30ëŒ€ ê°œë°œì (í•™ìŠµ ë°ì´í„°ì— ì—†ë˜ ì¡°í•©)
    "íŠ¹ì§•: 30ëŒ€ íŒêµ ê°œë°œì, ê´€ì‹¬ì‚¬: ìµœì‹  IT ê¸°ê¸°/ì–¼ë¦¬ì–´ë‹µí„°",
    
    # Case 2: ì€í‡´í•œ 60ëŒ€ (ìƒˆë¡œìš´ ì—°ë ¹ëŒ€)
    "íŠ¹ì§•: ì€í‡´í•œ 60ëŒ€ ë‚¨ì„±, ê´€ì‹¬ì‚¬: ë“±ì‚°/ë§‰ê±¸ë¦¬",
    
    # Case 3: ìœ¡ì•„ë§˜ (ë³µí•© ê´€ì‹¬ì‚¬)
    "íŠ¹ì§•: 30ëŒ€ ìœ¡ì•„ë§˜, ê´€ì‹¬ì‚¬: ìœ¡ì•„ìš©í’ˆ/ì¤‘ê³ ê±°ë˜"
]

# ==========================================
# 4. ì¶”ë¡  ì‹¤í–‰
# ==========================================
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
ìœ ì € ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ëª¨ì„ì„ ì¶”ì²œí•˜ê³  ì‚¬ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

### Input:
{}

### Response:
"""

print("\nğŸš€ ì¶”ì²œ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n" + "="*50)

for user_profile in test_users:
    inputs = tokenizer(
        [alpaca_prompt.format(user_profile)],
        return_tensors = "pt",
    ).to("cuda")

    outputs = model.generate(
        **inputs, 
        max_new_tokens = 256, # JSONë§Œ ë‚˜ì˜¤ë©´ ë˜ë‹ˆê¹Œ ê¸¸ì§€ ì•Šì•„ë„ ë¨
        use_cache = True,
        temperature = 0.1, # ì •ë‹µì— ê°€ê¹Œìš´ ì¶”ì²œì„ ìœ„í•´ ì°½ì˜ì„± ë‚®ì¶¤
    )
    
    # ê²°ê³¼ ë””ì½”ë”©
    result = tokenizer.batch_decode(outputs)
    final_output = result[0].split("### Response:")[-1].replace(tokenizer.eos_token, "").strip()
    
    print(f"\nğŸ‘¤ [User]: {user_profile}")
    print(f"ğŸ¤– [AI ì¶”ì²œ]: {final_output}")
    print("-" * 50)

print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")