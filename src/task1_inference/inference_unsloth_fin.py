# src/task1_inference/inference_unsloth_test.py

import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import json
import torch
from unsloth import FastLanguageModel
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# --- [ìµœì í™” ì„¤ì •] ---
LLM_MODEL_ID = "unsloth/Qwen3-14B-unsloth-bnb-4bit" # ë¡œì»¬ì— ì´ ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •

# 2. VRAM ì ˆì•½ í•µì‹¬ ì„¤ì •
# 14B ëª¨ë¸ ê¸°ì¤€: 1024(ì•ˆì „), 2048(ê¶Œì¥/16GB ì¶©ë¶„), 4096(ìœ„í—˜)
MAX_SEQ_LENGTH = 2048 
DTYPE = None # RTX 40ì‹œë¦¬ì¦ˆëŠ” ìë™ìœ¼ë¡œ bfloat16ì´ ì ìš©ë¨ (ê°€ì¥ ë¹ ë¦„)
LOAD_IN_4BIT = True # í•„ìˆ˜: ë©”ëª¨ë¦¬ë¥¼ 1/4ë¡œ ì¤„ì—¬ì¤Œ

EMBED_MODEL_ID = "BAAI/bge-m3"
INPUT_FILE = "./data/raw/dummy_users.json"
OUTPUT_FILE = "./data/processed/user_profiles_qwen3_result.json"

def main():
    print(f">>> [1/3] Unsloth Qwen 3 ë¡œë“œ ì¤‘... ({LLM_MODEL_ID})")

    # ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = LLM_MODEL_ID,
        max_seq_length = MAX_SEQ_LENGTH,
        dtype = DTYPE,
        load_in_4bit = LOAD_IN_4BIT,
        fix_tokenizer = True,
    )

    FastLanguageModel.for_inference(model)

    print(f">>> [2/3] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... ({EMBED_MODEL_ID})")
    embed_model = SentenceTransformer(EMBED_MODEL_ID, device="cuda")

    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            users = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: {INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f">>> [3/3] Qwen 3 ì¶”ë¡  ì‹œì‘ (Thinking Mode)...")
    processed_data = []
    
    # [ìˆ˜ì •] ëª¨ë¸ ë³¸ëŠ¥ì— ë§ëŠ” <think> íƒœê·¸ ì‚¬ìš© & ë‹«ëŠ” íƒœê·¸(</think>) ëª…ì‹œ
    system_prompt = """You are an AI assistant.
    First, think deeply about the user's hidden interests inside <think> tags.
    Then, output exactly 3 Korean keywords that best represent their persona.
    Format: <think> reasoning process... </think> í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"""

    for user in tqdm(users):
        items = user['items']
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"êµ¬ë§¤ ë¬¼í’ˆ: {items}\n\nê´€ì‹¬ì‚¬ëŠ”?"}
        ]
        
        inputs = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to("cuda")

        outputs = model.generate(
            input_ids=inputs, 
            max_new_tokens=256, # ìƒê°ì´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ì¡°ê¸ˆ ëŠ˜ë¦¼
            use_cache=True,
            temperature=0.3
        )
        
        raw_output = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0].strip()

        # [ìˆ˜ì •] ë“¤ì—¬ì“°ê¸° ì•ˆìœ¼ë¡œ ë„£ìŒ & </think> ë’¤ì— ìˆëŠ” í…ìŠ¤íŠ¸(ì •ë‹µ)ë§Œ ì¶”ì¶œ
        final_answer = raw_output
        thought_process = "..."

        if "</think>" in raw_output:
            parts = raw_output.split("</think>")
            thought_process = parts[0].replace("<think>", "").strip() # ìƒê° ë¶€ë¶„
            final_answer = parts[-1].strip() # ì •ë‹µ ë¶€ë¶„
        
        # ë¡œê·¸ì—ëŠ” ìƒê°ì˜ ì¼ë¶€ë§Œ ë³´ì—¬ì£¼ê³ , ì •ë‹µ ì¶œë ¥
        # print(f"ğŸ’¡ AI ìƒê°: {thought_process[:50]}...") 
        # print(f"âœ… ìµœì¢… í‚¤ì›Œë“œ: {final_answer}")

        vector = embed_model.encode(final_answer).tolist()

        processed_data.append({
            "user_id": user["user_id"],
            "items": items,
            "inferred_interests": final_answer,
            "thought_process": thought_process, # ë‚˜ì¤‘ì— ë¶„ì„ìš©ìœ¼ë¡œ ì €ì¥í•´ë‘ë©´ ì¢‹ìŒ
            "embedding": vector
        })

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)

    print(f">>> âœ… [Qwen 3 ì™„ë£Œ] ê²°ê³¼ ì €ì¥ë¨: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()