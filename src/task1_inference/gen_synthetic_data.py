import torch
from unsloth import FastLanguageModel
import json
import os
import gc
from tqdm import tqdm
import random

# ==========================================
# 1. ì„¤ì • (Configuration) - 16GB VRAM ìµœì í™”
# ==========================================
MODEL_ID = "unsloth/Qwen3-14B-unsloth-bnb-4bit" # â€» Qwen3ê°€ ì•„ì§ HFì— ì—†ë‹¤ë©´ 2.5 ì‚¬ìš© (User ì…ë ¥ëª…ì— ë§ê²Œ ìˆ˜ì • ê°€ëŠ¥)
MAX_SEQ_LENGTH = 1024 # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ 1024ë¡œ ì œí•œ (í”¼ë“œë°± ë°˜ì˜)
OUTPUT_FILE = "synthetic_train_data.jsonl"
TARGET_COUNT = 100 # ìš°ì„  100ê°œ ìƒì„± (Pilot Run)

# ëœë¤ì„±ì„ ìœ„í•œ ì‹œë“œ ë°ì´í„° (ë‹¤ì–‘ì„± í™•ë³´ìš©)
USER_SEGMENTS = ["20ëŒ€ ëŒ€í•™ìƒ", "30ëŒ€ ì§ì¥ì¸", "40ëŒ€ ì£¼ë¶€", "50ëŒ€ ìì˜ì—…ì", "ìƒˆë¡œ ì´ì‚¬ì˜¨ 1ì¸ ê°€êµ¬"]
INTERESTS = ["ìš´ë™/ë°°ë“œë¯¼í„´", "ë…ì„œ/ìê¸°ê³„ë°œ", "ë§›ì§‘íƒë°©", "ë°˜ë ¤ë™ë¬¼ ì‚°ì±…", "ë™ë„¤ ì¹œêµ¬ ë§Œë“¤ê¸°", "ì˜ì–´íšŒí™”"]

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ (Memory Safe Loading)
# ==========================================
print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_ID})")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = MODEL_ID,
    max_seq_length = MAX_SEQ_LENGTH,
    dtype = None, # Auto detection (bfloat16 for RTX 40 series)
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model) # ì¶”ë¡  ëª¨ë“œ ì „í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# ==========================================
# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (System Instruction)
# ==========================================
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
ë‹¹ì‹ ì€ ë‹¹ê·¼ë§ˆì¼“ì˜ AI ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 'ìœ ì € ì •ë³´'ë¥¼ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ìœ ì €ê°€ ê°€ì¥ ê´€ì‹¬ì„ ê°€ì§ˆë§Œí•œ 'ëª¨ì„(ë™ë„¤ìƒí™œ)'ì„ í•˜ë‚˜ ì¶”ì²œí•˜ê³ , ê·¸ ë…¼ë¦¬ì ì¸ ì¶”ì²œ ì‚¬ìœ ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ê²°ê³¼ëŠ” ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### Input:
{}

### Response:
"""

# ==========================================
# 4. ë°ì´í„° ìƒì„± ë£¨í”„
# ==========================================
print(f"ğŸš€ ë°ì´í„° ìƒì„± ì‹œì‘ (ëª©í‘œ: {TARGET_COUNT}ê°œ)")

# ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì´ì–´ì„œ ì‘ì„±í•˜ì§€ ì•Šê³ , ë®ì–´ì“¸ì§€ í™•ì¸ í•„ìš” (ì—¬ê¸°ì„  append ëª¨ë“œ ì‚¬ìš©)
mode = 'a' if os.path.exists(OUTPUT_FILE) else 'w'

with open(OUTPUT_FILE, mode, encoding='utf-8') as f:
    for i in tqdm(range(TARGET_COUNT)):
        
        # 4-1. ëœë¤ ì…ë ¥ ìƒì„± (ë‹¤ì–‘ì„± ì£¼ì…)
        user_profile = f"íŠ¹ì§•: {random.choice(USER_SEGMENTS)}, ê´€ì‹¬ì‚¬: {random.choice(INTERESTS)}"
        
        # 4-2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        inputs = tokenizer(
            [alpaca_prompt.format(user_profile)],
            return_tensors = "pt",
        ).to("cuda")

        # 4-3. ëª¨ë¸ ì¶”ë¡  (Generation)
        try:
            outputs = model.generate(
                **inputs,
                max_new_tokens = 512, # ì¶œë ¥ ê¸¸ì´ í™•ë³´
                use_cache = True,
                temperature = 0.8, # ì°½ì˜ì„± ë¶€ì—¬
            )
            
            # 4-4. ê²°ê³¼ ë””ì½”ë”© ë° ì €ì¥
            decoded_output = tokenizer.batch_decode(outputs)
            response_text = decoded_output[0].split("### Response:")[-1].replace("<|endoftext|>", "").strip()
            
            # JSONL í¬ë§·ìœ¼ë¡œ ì €ì¥ (Instruction Tuningìš© êµ¬ì¡°)
            data_entry = {
                "instruction": "ìœ ì € ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ëª¨ì„ì„ ì¶”ì²œí•˜ê³  ì‚¬ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
                "input": user_profile,
                "output": response_text
            }
            f.write(json.dumps(data_entry, ensure_ascii=False) + "\n")
            
        except Exception as e:
            print(f"âš ï¸ Error at index {i}: {e}")

        # 4-5. ë©”ëª¨ë¦¬ ê´€ë¦¬ (ë§¤ìš° ì¤‘ìš” - VRAM ëˆ„ìˆ˜ ë°©ì§€)
        del inputs, outputs
        if i % 10 == 0: # 10ë²ˆë§ˆë‹¤ ìºì‹œ ë¹„ìš°ê¸°
            torch.cuda.empty_cache()
            gc.collect()

print(f"ğŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ! íŒŒì¼ ì €ì¥ë¨: {OUTPUT_FILE}")