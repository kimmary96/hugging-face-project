import json
import os
from unsloth import FastLanguageModel
from tqdm import tqdm

# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================
TARGET_COUNT = 1000  # ëª©í‘œ ë°ì´í„° ê°œìˆ˜
OUTPUT_FILE = "synthetic_data_1000.jsonl"
MODEL_NAME = "unsloth/Qwen3-14B-unsloth-bnb-4bit" # Teacher ëª¨ë¸

# ==========================================
# 1. ëª¨ë¸ ë¡œë“œ (Teacher)
# ==========================================
print(f"ğŸ”„ Teacher ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_NAME})")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = MODEL_NAME,
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

# ==========================================
# 2. ì´ì–´í•˜ê¸° í™•ì¸ (Resume)
# ==========================================
current_count = 0
if os.path.exists(OUTPUT_FILE):
    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
        current_count = sum(1 for line in f)
    print(f"ğŸ“‚ ê¸°ì¡´ íŒŒì¼ ë°œê²¬: {current_count}ê°œ ìƒì„±ë¨. ì´ì–´ì„œ ìƒì„±í•©ë‹ˆë‹¤.")

# ==========================================
# 3. ë°ì´í„° ìƒì„± ë£¨í”„
# ==========================================
prompt_template = """ë‹¹ì‹ ì€ ë‹¹ê·¼ë§ˆì¼“ì˜ AI ì¶”ì²œ ë°ì´í„° ìƒì„±ê¸°ì…ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ìœ ì € í˜ë¥´ì†Œë‚˜(ë‚˜ì´, ì§ì—…, ê´€ì‹¬ì‚¬, ìƒí™©)ë¥¼ ê°€ì •í•˜ê³ , ê·¸ì— ë§ëŠ” ë™ë„¤ìƒí™œ ëª¨ì„ ì¶”ì²œ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
ì°½ì˜ì ì´ê³  êµ¬ì²´ì ì¸ ìƒí™©ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
{"instruction": "ìœ ì € ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ëª¨ì„ì„ ì¶”ì²œí•˜ê³  ì‚¬ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.", "input": "íŠ¹ì§•: {ìœ ì €íŠ¹ì§•}, ê´€ì‹¬ì‚¬: {ê´€ì‹¬ì‚¬}", "output": "{\\"recommendation\\": \\"{ì¶”ì²œëª¨ì„ëª…}\\", \\"reasoning\\": \\"{ë…¼ë¦¬ì ì¸ ì¶”ì²œ ì‚¬ìœ }\\"}"}
"""

print(f"ğŸš€ ë°ì´í„° ìƒì„± ì‹œì‘! (ëª©í‘œ: {TARGET_COUNT}ê°œ)")

# íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ë’¤ì— ì´ì–´ë¶™ì„ ('a')
with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
    pbar = tqdm(total=TARGET_COUNT, initial=current_count)
    
    while current_count < TARGET_COUNT:
        try:
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt_template}
            ]
            inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")

            outputs = model.generate(
                inputs, 
                max_new_tokens=512,
                temperature=0.8, # ì°½ì˜ì„± ë¶€ì—¬
                do_sample=True,
                use_cache=True
            )
            
            decoded = tokenizer.batch_decode(outputs)
            response = decoded[0].split("<|im_start|>assistant")[-1].replace("<|im_end|>", "").strip()
            
            # JSON í¬ë§· ê²€ì¦ì€ ë‚˜ì¤‘ì— ì •ì œ ë‹¨ê³„ì—ì„œ í•˜ë¯€ë¡œ ì¼ë‹¨ ì €ì¥
            if "{" in response and "}" in response:
                # ê°€ì§œ instruction êµ¬ì¡°ë¡œ ë˜í•‘í•˜ì—¬ ì €ì¥ (í•™ìŠµ í¬ë§· ë§ì¶¤)
                # ì‹¤ì œ ëª¨ë¸ ì¶œë ¥ì—ì„œ JSON ë¶€ë¶„ë§Œ íŒŒì‹±í•´ì„œ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜, 
                # ì—¬ê¸°ì„œëŠ” Teacherê°€ ì¶œë ¥í•œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ raw ë°ì´í„°ë¡œ ì €ì¥.
                # ì •ì œ ìŠ¤í¬ë¦½íŠ¸ê°€ ì´ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•¨.
                entry = {"raw_output": response} 
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                f.flush() # ì¦‰ì‹œ ë””ìŠ¤í¬ ê¸°ë¡
                
                current_count += 1
                pbar.update(1)
                
        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
            continue

print("âœ¨ ë°ì´í„° ìƒì„± ì™„ë£Œ!")