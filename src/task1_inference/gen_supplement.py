"""
ì¶”ê°€ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (gen_supplement.py)

Qwen3-14Bë¥¼ ì‚¬ìš©í•˜ì—¬ ì•½ì  ë³´ì™„ìš© Hard Case ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- Model: unsloth/Qwen3-14B-unsloth-bnb-4bit
- Mode: Thinking Mode (ê¸°ë³¸ê°’)
- ëª©í‘œ: 300ê°œ Hard Case ìƒì„±
"""

import torch
from unsloth import FastLanguageModel
import json
import os
import gc
from tqdm import tqdm
import re

# ==========================================
# 1. ì„¤ì • (Configuration) - 16GB VRAM ìµœì í™”
# ==========================================
MODEL_ID = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
MAX_SEQ_LENGTH = 2048  # Thinking ModeëŠ” í† í°ì´ ë” í•„ìš”
OUTPUT_FILE = "data/supplement_data_300.jsonl"

# ìƒì„± ì„¤ì • (Thinking Mode ê¶Œì¥)
GENERATION_CONFIG = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
    "use_cache": True,
}

# ==========================================
# 2. íƒ€ê²Ÿ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ (ì•½ì  ë³´ì™„ìš©)
# ==========================================
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ 'ë‹¹ê·¼ë§ˆì¼“ êµ¬ë§¤ íŒ¨í„´ ìƒì„±ê¸°'ì…ë‹ˆë‹¤.
í˜„ì¬ AI ëª¨ë¸ì´ 'ì·¨ë¯¸', 'ìê¸°ê³„ë°œ', 'ìœ¡ì•„', 'ë°˜ë ¤ë™ë¬¼'ì„ ì„œë¡œ í—·ê°ˆë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì´ ê²½ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” **'ì–´ë ¤ìš´(Hard)' ì¼€ì´ìŠ¤**ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ 3ê°€ì§€ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ê· í˜• ìˆê²Œ ìƒì„±í•˜ì„¸ìš” (JSON í˜•ì‹):

1. **[ìœ í˜• A: ìê¸°ê³„ë°œ vs ì·¨ë¯¸]**
   - ê²‰ë³´ê¸°ì—” ì·¨ë¯¸ ê°™ì§€ë§Œ, ì‹¤ì œë¡œëŠ” 'ìê²©ì¦', 'ì·¨ì—…', 'ë¶€ì—…', 'ëŒ€íšŒ' ëª©ì ì´ ëšœë ·í•œ ë¬¼ê±´ ëª©ë¡.
   - ì˜ˆ: (ë°”ë¦¬ìŠ¤íƒ€ ìê²©ì¦ ì±… + ì•ì¹˜ë§ˆ -> ìê¸°ê³„ë°œ), (GTQ ìˆ˜í—˜ì„œ + í¬í† ìƒµ -> ìê¸°ê³„ë°œ)

2. **[ìœ í˜• B: ìœ¡ì•„ vs ì·¨ë¯¸/ì‚´ë¦¼]**
   - ìš”ë¦¬ë‚˜ ë§Œë“¤ê¸°ë¥¼ í•˜ì§€ë§Œ, ëª©ì ì´ ì² ì €íˆ 'ì•„ì´ë¥¼ ìœ„í•œ' ê²ƒì¸ ëª©ë¡.
   - ì˜ˆ: (ìœ ê¸°ë† ë°€ê°€ë£¨ + ìºë¦­í„° ì¿ í‚¤í‹€ + ì•„ì´ ê°„ì‹ í¬ì¥ì§€ -> ìœ¡ì•„)

3. **[ìœ í˜• C: íŠ¹ìˆ˜ ë°˜ë ¤ë™ë¬¼]**
   - ê°œ/ê³ ì–‘ì´ê°€ ì•„ë‹Œ ê³¤ì¶©, íŒŒì¶©ë¥˜, ì†Œë™ë¬¼, ë¬¼ê³ ê¸° ê´€ë ¨ ìš©í’ˆ.
   - ì˜ˆ: (ë°€ì›œ + í†±ë°¥ + ì‚¬ìœ¡í†µ -> ë°˜ë ¤ë™ë¬¼)

**ì¶œë ¥ í˜•ì‹ (JSON Only, í•œ ì¤„ì”©):**
{{"instruction": "ìœ ì €ì˜ ì´¬ì˜ ë¬¼ê±´ê³¼ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ëª¨ì„ ì¹´í…Œê³ ë¦¬ì™€ ë¶„ìœ„ê¸°ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.", "input": "í˜ë¥´ì†Œë‚˜: [í˜ë¥´ì†Œë‚˜], ë¬¼ê±´ëª©ë¡: [[ë¬¼ê±´1], [ë¬¼ê±´2], ...], í‰ê· ê°€ê²©: [ê°€ê²©ëŒ€], ì´¬ì˜ë¹ˆë„: [ë¹ˆë„]", "output": "{{\\"category\\":\\"[ì¹´í…Œê³ ë¦¬]\\",\\"hard_negative\\":{{\\"confusing\\":\\"[í˜¼ë™ì¹´í…Œê³ ë¦¬]\\",\\"reason\\":\\"[ì™œ í˜¼ë™ë  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…]\\"}}}}"}}

ìœ„ í˜•ì‹ì˜ JSONì„ ì •í™•íˆ {count}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”. ê° JSONì€ ë³„ë„ì˜ ì¤„ì— ì¶œë ¥í•˜ì„¸ìš”."""


def load_model():
    """Qwen3-14B ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_ID})")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_ID,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,  # RTX 40ì‹œë¦¬ì¦ˆëŠ” ìë™ìœ¼ë¡œ bfloat16 ì ìš©
        load_in_4bit=True,
    )

    FastLanguageModel.for_inference(model)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    return model, tokenizer


def parse_thinking_output(raw_output: str) -> str:
    """Thinking Mode ì¶œë ¥ì—ì„œ <think>...</think> ì œê±° í›„ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ"""
    if "</think>" in raw_output:
        parts = raw_output.split("</think>")
        return parts[-1].strip()
    return raw_output.strip()


def extract_json_lines(text: str) -> list:
    """í…ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ JSON ë¼ì¸ë“¤ ì¶”ì¶œ"""
    results = []

    # ì½”ë“œ ë¸”ë¡ ì œê±°
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)

    # ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for line in text.split('\n'):
        line = line.strip()
        if not line or not line.startswith('{'):
            continue

        try:
            obj = json.loads(line)
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if 'instruction' in obj and 'input' in obj and 'output' in obj:
                results.append(obj)
        except json.JSONDecodeError:
            # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
            start = line.find('{')
            end = line.rfind('}')
            if start != -1 and end != -1:
                try:
                    obj = json.loads(line[start:end+1])
                    if 'instruction' in obj and 'input' in obj and 'output' in obj:
                        results.append(obj)
                except json.JSONDecodeError:
                    continue

    return results


def generate_batch(model, tokenizer, count: int = 5) -> list:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ìƒì„±"""
    user_prompt = SYSTEM_PROMPT.format(count=count)

    messages = [
        {"role": "system", "content": "You are a helpful data generation assistant."},
        {"role": "user", "content": user_prompt}
    ]

    # Thinking Mode (ê¸°ë³¸ê°’): enable_thinking íŒŒë¼ë¯¸í„° ì—†ì´ ì‚¬ìš©
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    # ìƒì„±
    outputs = model.generate(
        input_ids=inputs,
        **GENERATION_CONFIG
    )

    # ë””ì½”ë”©
    raw_output = tokenizer.decode(
        outputs[0][inputs.shape[1]:],
        skip_special_tokens=True
    )

    # Thinking ë¸”ë¡ ì œê±°
    final_output = parse_thinking_output(raw_output)

    # JSON ë¼ì¸ ì¶”ì¶œ
    items = extract_json_lines(final_output)

    return items


def main():
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()

    # ì´ì–´í•˜ê¸° í™•ì¸ (Resume)
    current_count = 0
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            current_count = sum(1 for line in f if line.strip())
        print(f"ğŸ“‚ ê¸°ì¡´ íŒŒì¼ ë°œê²¬: {current_count}ê°œ ìƒì„±ë¨. ì´ì–´ì„œ ìƒì„±í•©ë‹ˆë‹¤.")

    # ì„¤ì •
    total_target = 300
    batch_size = 5  # í•œ ë²ˆì— ìš”ì²­í•  ê°œìˆ˜

    print(f"\nğŸš€ ë°ì´í„° ìƒì„± ì‹œì‘! (ëª©í‘œ: {total_target}ê°œ)")

    # ìƒì„± ë£¨í”„
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        pbar = tqdm(total=total_target, initial=current_count, desc="ìƒì„± ì¤‘")

        while current_count < total_target:
            try:
                items = generate_batch(model, tokenizer, batch_size)

                for item in items:
                    if current_count >= total_target:
                        break

                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
                    f.flush()
                    current_count += 1
                    pbar.update(1)

            except Exception as e:
                print(f"\nâš ï¸ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
                continue

        pbar.close()

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()

    print(f"\nâœ… ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {OUTPUT_FILE}")
    print(f"ì´ {current_count}ê°œ ë°ì´í„° ìƒì„±ë¨")


if __name__ == "__main__":
    main()
