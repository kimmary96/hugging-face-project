import torch
from unsloth import FastLanguageModel
import os

# ==========================================
# 1. í™˜ê²½ ë° ê²½ë¡œ ì„¤ì •
# ==========================================
max_seq_length = 2048 # ë§¤ì¹­ ë‹¨ê³„ì—ì„œëŠ” ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ íš¨ìœ¨ì„± ìœ„í•´ ì¡°ì ˆ
dtype = None 
load_in_4bit = True 

# ìµœì¢… í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Day 2 ë°¤ìƒ˜ ì‘ì—… ê²°ê³¼ë¬¼)
# ë§Œì•½ ê²½ë¡œ ì—ëŸ¬ ë°œìƒ ì‹œ ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš” (ì˜ˆ: "C:/hugging-face-project/outputs_final_1000")
model_path = "C:\\Users\\User\\Documents\\dev\\hugging-face-project\\outputs_experiments\\Exp2_Overfitting\\checkpoint-49"

print(f"ğŸ”„ [System] ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... ê²½ë¡œ: {model_path}")

# ==========================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==========================================
try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_path, # ì—¬ê¸°ì„œ í•™ìŠµëœ ì–´ëŒ‘í„°ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
        # device_map="auto" # UnslothëŠ” ê¸°ë³¸ì ìœ¼ë¡œ GPU 0ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
    )
    FastLanguageModel.for_inference(model) # ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ (Native ìœˆë„ìš° í•„ìˆ˜)
    print("âœ… [Success] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (RTX 4070 Ti Super / 16GB VRAM Optimized)")

except Exception as e:
    print(f"âŒ [Error] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("Tip: \"outputs_experiment/checkpoint-125\" í´ë” ì•ˆì— 'adapter_model.bin' í˜¹ì€ 'safetensors'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# ==========================================
# 3. ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Inference Test)
# ==========================================
# ì‹œë‚˜ë¦¬ì˜¤: Day 2 Baselineë³´ë‹¤ ë³µì¡í•œ, ìˆ¨ê²¨ì§„ ì˜ë„ë¥¼ íŒŒì•…í•´ì•¼ í•˜ëŠ” ìœ ì €
test_input = "ìµœê·¼ì— 'ì•„ê¸°ë 'ë‘ 'ë°©ìˆ˜ ì‹íƒë³´'ë¥¼ ìƒ€ê³ , ì–´ì œëŠ” 'ì†ëª© ë³´í˜¸ëŒ€'ë¥¼ ê²€ìƒ‰í–ˆì–´. ë‚˜í•œí…Œ ë§ëŠ” ëª¨ì„ ì—†ì„ê¹Œ?"

# í”„ë¡¬í”„íŠ¸ í¬ë§· (Alpaca ìŠ¤íƒ€ì¼ - í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
ì‚¬ìš©ìì˜ êµ¬ë§¤ ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ¨ê²¨ì§„ ê´€ì‹¬ì‚¬ì™€ í˜ë¥´ì†Œë‚˜ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.

### Input:
{test_input}

### Response:
"""

print("\nğŸ¤– [AI Thinking] ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")

inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")

# ìƒì„± ì˜µì…˜ ì„¤ì •
outputs = model.generate(
    **inputs, 
    max_new_tokens = 256, 
    use_cache = True,
    temperature = 0.7, # ì°½ì˜ì„±ê³¼ ì •í™•ì„±ì˜ ê· í˜•
)

# ê²°ê³¼ ë””ì½”ë”©
decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens = True)
result = decoded_output[0].split("### Response:")[-1].strip()

print("="*50)
print(f"ğŸ“ [User Input]: {test_input}")
print("-" * 50)
print(f"ğŸ’¡ [Model Output (Persona Analysis)]:\n{result}")
print("="*50)