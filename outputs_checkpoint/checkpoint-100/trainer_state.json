{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 11.121212121212121,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 2.73006534576416,
      "learning_rate": 0.0,
      "loss": 2.1965,
      "step": 1
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 3.097433090209961,
      "learning_rate": 2e-05,
      "loss": 2.2733,
      "step": 2
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 3.021831750869751,
      "learning_rate": 4e-05,
      "loss": 2.2667,
      "step": 3
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 3.1707637310028076,
      "learning_rate": 6e-05,
      "loss": 2.2624,
      "step": 4
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.191293716430664,
      "learning_rate": 8e-05,
      "loss": 2.0784,
      "step": 5
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.860094666481018,
      "learning_rate": 0.0001,
      "loss": 1.8959,
      "step": 6
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.7279093265533447,
      "learning_rate": 0.00012,
      "loss": 1.7739,
      "step": 7
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.1787909269332886,
      "learning_rate": 0.00014,
      "loss": 1.5959,
      "step": 8
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0371695756912231,
      "learning_rate": 0.00016,
      "loss": 1.4564,
      "step": 9
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.9972089529037476,
      "learning_rate": 0.00018,
      "loss": 1.2823,
      "step": 10
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.8454465270042419,
      "learning_rate": 0.0002,
      "loss": 1.17,
      "step": 11
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.6571363210678101,
      "learning_rate": 0.00019777777777777778,
      "loss": 1.054,
      "step": 12
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.5499305129051208,
      "learning_rate": 0.00019555555555555556,
      "loss": 1.0007,
      "step": 13
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.619861364364624,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.8974,
      "step": 14
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.5115661025047302,
      "learning_rate": 0.00019111111111111114,
      "loss": 0.8175,
      "step": 15
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 0.9625976085662842,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.7594,
      "step": 16
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.5623365640640259,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.6918,
      "step": 17
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.6473935842514038,
      "learning_rate": 0.00018444444444444446,
      "loss": 0.8667,
      "step": 18
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 1.4939643144607544,
      "learning_rate": 0.00018222222222222224,
      "loss": 0.6614,
      "step": 19
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.4164556860923767,
      "learning_rate": 0.00018,
      "loss": 0.6538,
      "step": 20
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.40445971488952637,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.6232,
      "step": 21
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 0.351787269115448,
      "learning_rate": 0.00017555555555555556,
      "loss": 0.64,
      "step": 22
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.3272622227668762,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.5983,
      "step": 23
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.36118611693382263,
      "learning_rate": 0.0001711111111111111,
      "loss": 0.5299,
      "step": 24
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.32683008909225464,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.512,
      "step": 25
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.30559492111206055,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.5012,
      "step": 26
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.5346662998199463,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.4479,
      "step": 27
    },
    {
      "epoch": 3.121212121212121,
      "grad_norm": 0.2667772173881531,
      "learning_rate": 0.00016222222222222224,
      "loss": 0.4967,
      "step": 28
    },
    {
      "epoch": 3.242424242424242,
      "grad_norm": 0.3167957663536072,
      "learning_rate": 0.00016,
      "loss": 0.5071,
      "step": 29
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.282088965177536,
      "learning_rate": 0.0001577777777777778,
      "loss": 0.4862,
      "step": 30
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 0.29440057277679443,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.4643,
      "step": 31
    },
    {
      "epoch": 3.606060606060606,
      "grad_norm": 0.2672702968120575,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.4239,
      "step": 32
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 0.2779863774776459,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.478,
      "step": 33
    },
    {
      "epoch": 3.8484848484848486,
      "grad_norm": 0.3333113193511963,
      "learning_rate": 0.0001488888888888889,
      "loss": 0.4496,
      "step": 34
    },
    {
      "epoch": 3.9696969696969697,
      "grad_norm": 0.32847610116004944,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.4576,
      "step": 35
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.5684131979942322,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.5009,
      "step": 36
    },
    {
      "epoch": 4.121212121212121,
      "grad_norm": 0.2722448408603668,
      "learning_rate": 0.00014222222222222224,
      "loss": 0.428,
      "step": 37
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 0.28270959854125977,
      "learning_rate": 0.00014,
      "loss": 0.377,
      "step": 38
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.2972155809402466,
      "learning_rate": 0.0001377777777777778,
      "loss": 0.4136,
      "step": 39
    },
    {
      "epoch": 4.484848484848484,
      "grad_norm": 0.2817705273628235,
      "learning_rate": 0.00013555555555555556,
      "loss": 0.3891,
      "step": 40
    },
    {
      "epoch": 4.606060606060606,
      "grad_norm": 0.2578246295452118,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3859,
      "step": 41
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.290805459022522,
      "learning_rate": 0.00013111111111111111,
      "loss": 0.4004,
      "step": 42
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.30797278881073,
      "learning_rate": 0.00012888888888888892,
      "loss": 0.3926,
      "step": 43
    },
    {
      "epoch": 4.96969696969697,
      "grad_norm": 0.3017009198665619,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.4038,
      "step": 44
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.5455127358436584,
      "learning_rate": 0.00012444444444444444,
      "loss": 0.4983,
      "step": 45
    },
    {
      "epoch": 5.121212121212121,
      "grad_norm": 0.27554449439048767,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.3737,
      "step": 46
    },
    {
      "epoch": 5.242424242424242,
      "grad_norm": 0.26687589287757874,
      "learning_rate": 0.00012,
      "loss": 0.377,
      "step": 47
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.2945120930671692,
      "learning_rate": 0.00011777777777777779,
      "loss": 0.3532,
      "step": 48
    },
    {
      "epoch": 5.484848484848484,
      "grad_norm": 0.2933741509914398,
      "learning_rate": 0.00011555555555555555,
      "loss": 0.3691,
      "step": 49
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 0.31054556369781494,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.3418,
      "step": 50
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.35214775800704956,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.3532,
      "step": 51
    },
    {
      "epoch": 5.848484848484849,
      "grad_norm": 0.3564300835132599,
      "learning_rate": 0.00010888888888888889,
      "loss": 0.3692,
      "step": 52
    },
    {
      "epoch": 5.96969696969697,
      "grad_norm": 0.4124397337436676,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.3529,
      "step": 53
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.5143124461174011,
      "learning_rate": 0.00010444444444444445,
      "loss": 0.3027,
      "step": 54
    },
    {
      "epoch": 6.121212121212121,
      "grad_norm": 0.31946703791618347,
      "learning_rate": 0.00010222222222222222,
      "loss": 0.3369,
      "step": 55
    },
    {
      "epoch": 6.242424242424242,
      "grad_norm": 0.27686309814453125,
      "learning_rate": 0.0001,
      "loss": 0.2934,
      "step": 56
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.3373505771160126,
      "learning_rate": 9.777777777777778e-05,
      "loss": 0.354,
      "step": 57
    },
    {
      "epoch": 6.484848484848484,
      "grad_norm": 0.2869994044303894,
      "learning_rate": 9.555555555555557e-05,
      "loss": 0.3348,
      "step": 58
    },
    {
      "epoch": 6.606060606060606,
      "grad_norm": 0.3241330683231354,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.3213,
      "step": 59
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.2819938063621521,
      "learning_rate": 9.111111111111112e-05,
      "loss": 0.2817,
      "step": 60
    },
    {
      "epoch": 6.848484848484849,
      "grad_norm": 0.3027552664279938,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.314,
      "step": 61
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 0.30637872219085693,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.3537,
      "step": 62
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.6326596140861511,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.3675,
      "step": 63
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 0.26934507489204407,
      "learning_rate": 8.222222222222222e-05,
      "loss": 0.2901,
      "step": 64
    },
    {
      "epoch": 7.242424242424242,
      "grad_norm": 0.28952455520629883,
      "learning_rate": 8e-05,
      "loss": 0.2956,
      "step": 65
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.31886157393455505,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.2759,
      "step": 66
    },
    {
      "epoch": 7.484848484848484,
      "grad_norm": 0.31352776288986206,
      "learning_rate": 7.555555555555556e-05,
      "loss": 0.3274,
      "step": 67
    },
    {
      "epoch": 7.606060606060606,
      "grad_norm": 0.3245236277580261,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.2993,
      "step": 68
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.28265079855918884,
      "learning_rate": 7.111111111111112e-05,
      "loss": 0.3128,
      "step": 69
    },
    {
      "epoch": 7.848484848484849,
      "grad_norm": 0.3321024179458618,
      "learning_rate": 6.88888888888889e-05,
      "loss": 0.281,
      "step": 70
    },
    {
      "epoch": 7.96969696969697,
      "grad_norm": 0.3044169843196869,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.2847,
      "step": 71
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.6592963337898254,
      "learning_rate": 6.444444444444446e-05,
      "loss": 0.2704,
      "step": 72
    },
    {
      "epoch": 8.121212121212121,
      "grad_norm": 0.3001324236392975,
      "learning_rate": 6.222222222222222e-05,
      "loss": 0.2424,
      "step": 73
    },
    {
      "epoch": 8.242424242424242,
      "grad_norm": 0.3009210526943207,
      "learning_rate": 6e-05,
      "loss": 0.2802,
      "step": 74
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.335414856672287,
      "learning_rate": 5.7777777777777776e-05,
      "loss": 0.2961,
      "step": 75
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 0.3431578278541565,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.3198,
      "step": 76
    },
    {
      "epoch": 8.606060606060606,
      "grad_norm": 0.3148852288722992,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.2495,
      "step": 77
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.31416112184524536,
      "learning_rate": 5.111111111111111e-05,
      "loss": 0.2544,
      "step": 78
    },
    {
      "epoch": 8.848484848484848,
      "grad_norm": 0.3331397473812103,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.2779,
      "step": 79
    },
    {
      "epoch": 8.969696969696969,
      "grad_norm": 0.3191035985946655,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.2409,
      "step": 80
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.6634337306022644,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.3037,
      "step": 81
    },
    {
      "epoch": 9.121212121212121,
      "grad_norm": 0.3126490116119385,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.278,
      "step": 82
    },
    {
      "epoch": 9.242424242424242,
      "grad_norm": 0.3374811112880707,
      "learning_rate": 4e-05,
      "loss": 0.268,
      "step": 83
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.32647931575775146,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.2304,
      "step": 84
    },
    {
      "epoch": 9.484848484848484,
      "grad_norm": 0.3022964894771576,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.242,
      "step": 85
    },
    {
      "epoch": 9.606060606060606,
      "grad_norm": 0.36820003390312195,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.2298,
      "step": 86
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.30770018696784973,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.2381,
      "step": 87
    },
    {
      "epoch": 9.848484848484848,
      "grad_norm": 0.30505090951919556,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.2682,
      "step": 88
    },
    {
      "epoch": 9.969696969696969,
      "grad_norm": 0.32128262519836426,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.2619,
      "step": 89
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.7794439196586609,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.2354,
      "step": 90
    },
    {
      "epoch": 10.121212121212121,
      "grad_norm": 0.3127323389053345,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.1961,
      "step": 91
    },
    {
      "epoch": 10.242424242424242,
      "grad_norm": 0.3158826231956482,
      "learning_rate": 2e-05,
      "loss": 0.2418,
      "step": 92
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.3112224340438843,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.2328,
      "step": 93
    },
    {
      "epoch": 10.484848484848484,
      "grad_norm": 0.31794968247413635,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.2229,
      "step": 94
    },
    {
      "epoch": 10.606060606060606,
      "grad_norm": 0.3374496102333069,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.2623,
      "step": 95
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.3391762971878052,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.256,
      "step": 96
    },
    {
      "epoch": 10.848484848484848,
      "grad_norm": 0.32355204224586487,
      "learning_rate": 8.88888888888889e-06,
      "loss": 0.2259,
      "step": 97
    },
    {
      "epoch": 10.969696969696969,
      "grad_norm": 0.3303515315055847,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.2651,
      "step": 98
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.7421334385871887,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.2594,
      "step": 99
    },
    {
      "epoch": 11.121212121212121,
      "grad_norm": 0.33160704374313354,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 0.2087,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6116210175237120.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
